# FastAPI Train API + Data Pipeline (Extract â†’ Clean â†’ Upload)
import logging 
from fastapi import FastAPI
from contextlib import asynccontextmanager

# Project modules for the data pipeline
from data_pipeline.extract import extract_data
from data_pipeline.clean import process_data
from data_pipeline.merge import merge_geospatial_data
from data_pipeline.utils import upload_to_db
from data_pipeline.map import generate_html_map

# Set up logging 
logger = logging.getLogger("train_api")
logging.basicConfig(level=logging.INFO, format="%(levelname)s: %(message)s")

# âœ… Default station & date
STATION = "FPK"  # Finsbury Park (arrivals)
DAYS = 7

# âœ… Lifespan function to run data pipeline before FastAPI starts
@asynccontextmanager
async def lifespan(app: FastAPI):
    logger.info("ğŸš€ Running data pipeline before starting API...")
    
    # âœ… Step 1 : Extract live data 
    logger.info("ğŸš€ Extracting live data for station %s over the past %d days...", STATION, DAYS)
    raw_data = extract_data(STATION, DAYS)
    
    if not raw_data or "services" not in raw_data:
        logger.error("âŒ No valid raw data found. Skipping further processing.")
        yield
        return  # Stop execution if no data
    
    # âœ… 2 Step 2:  Process & clean extracted data    
    logger.info("Starting processing and cleaning of extracted data...")
    df_clean = process_data(raw_data)
    
    if df_clean.empty:
        logger.error("âŒ No valid train records after processing. Skipping upload.")
        yield
        return  # Stop execution if no clean data
    
    # âœ… Step 3: Merge with geospatial data 
    logger.info("ğŸš€ Merging geospatial data with cleaned train records...")
    df_merged = merge_geospatial_data(df_clean)
    if df_merged.empty:
        logger.error("âŒ No valid train records after merging geospatial data.")
        yield
        return
     
    # âœ… Step 4: Upload cleaned data to PostgreSQL
    logger.info("ğŸš€ Running database upload for %d records...", len(df_merged))
    await upload_to_db(df_merged)
    
    # âœ… Step 5: Generate HTML Map showing delays
    logger.info("ğŸš€ Generating train delay map...")
    map_path = generate_html_map(df_merged)
    logger.info("âœ… Map Saved to %s", map_path)

    logger.info("âœ… Data pipeline completed successfully!")
    yield  # âœ… Allows API to start after setup

# âœ… Create FastAPI app with lifespan
app = FastAPI(lifespan=lifespan)


"""
# âœ… Example FastAPI endpoints for monitoring purposes
@app.get("/")
def home():
    return {"message": "Train Delay API is running!"}

# TODO: Example dynamic API endpoint using parameters
@app.get("/api/station/{station}/date/{date}")
def get_trains(station: str, date: str):
    
    Example API endpoint to retrieve train data for a specific station and date.
    
    raw_data = extract_data(station, date)
    return raw_data if raw_data else {"error": "No data found"}

    """
# âœ… Run FastAPI server (if executed directly)
if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="127.0.0.1", port=8000, reload=True)